{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDjZmCQ5skbV"
      },
      "source": [
        "!pip install -q tf-models-official==2.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufBVp5_EbvHJ",
        "outputId": "6b2a469f-5d51-450d-f2ef-12694b42bd81"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from scipy import stats\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "import datetime, os\n",
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.patches as mpatches\n",
        "from IPython.display import display, Image, clear_output\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import regularizers\n",
        "import math\n",
        "import h5py\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from official import nlp\n",
        "import official.nlp.optimization\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n",
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['copy']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MozLQRcetBDV"
      },
      "source": [
        "# keras "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTo7CP6IB2fk"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=8000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myScheduler():\n",
        "    def __init__(self, goal_lr=1e-4, end_lr=1e-5, beta=1e-4, n_epochs=100, w_epochs=10):\n",
        "        self.goal_lr = goal_lr\n",
        "        self.end_lr  = end_lr\n",
        "        self.alpha   = goal_lr/w_epochs\n",
        "        self.beta    = beta\n",
        "        self.lr      = goal_lr/w_epochs\n",
        "        self.w_epochs= w_epochs\n",
        "    def get_lr(self, epoch):\n",
        "        if(epoch<self.w_epochs):\n",
        "            self.lr += self.alpha\n",
        "        else:\n",
        "            self.lr -= (np.exp(self.beta*epoch)-self.end_lr)\n",
        "        return self.lr\n",
        "\n",
        "# s = myScheduler(1e-4,\n",
        "#                 1e-5,\n",
        "#                 1e-4,\n",
        "#                 100,\n",
        "#                 20)\n",
        "# x = np.array([i for i in range(100)])\n",
        "# y = np.array([s.get_lr(i) for i in x])\n",
        "\n",
        "# sns.lineplot(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "gAzP5Bp9QKWV",
        "outputId": "67b3b11f-3203-4d00-cd49-2a2b2b53fe15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcda07a3810>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XTWRzMjj4KF"
      },
      "source": [
        "def get_angles(pos, i, d_model, c1=10_000):\n",
        "  angle_rates = 1 / np.power(c1, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgnszfl-j4Ai"
      },
      "source": [
        "def positional_encoding(position, d_model, c1=10_000):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model,\n",
        "                          c1)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReUsT7i5xwS1"
      },
      "source": [
        "def tss_positional_encoding(position, d_model, tss):\n",
        "#   angle_rads = get_angles(np.abs(np.arange(position)-tss)[:, np.newaxis],\n",
        "#                           np.arange(d_model)[np.newaxis, :],\n",
        "#                           d_model)\n",
        "    angle_rads = get_angles((np.arange(position)-tss)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1cSVBSfx8Vm"
      },
      "source": [
        "# tpe = tss_positional_encoding(666, 32, 666//2)\n",
        "# fig, ax = plt.subplots(figsize=(100,50))\n",
        "# ax.matshow(tpe[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4KGPxN8cIru"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, p_enc=\"w2v\"):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.p_enc = p_enc\n",
        "\n",
        "    def call(self, inputs, masked=None, training=None):\n",
        "        if self.p_enc == \"relative\":\n",
        "            x, x_q, x_k, x_v = inputs\n",
        "            if masked is not None:\n",
        "                attn_output, att_scores = self.att(x_q, x_k, x_v, return_attention_scores=True, attention_mask=masked)\n",
        "            else:\n",
        "                attn_output, att_scores = self.att(x_q, x_k, x_v, return_attention_scores=True)\n",
        "            inputs = x\n",
        "        else:\n",
        "            if masked is not None:\n",
        "                attn_output, att_scores = self.att(inputs, inputs, inputs, return_attention_scores=True, attention_mask=masked)\n",
        "            else:\n",
        "                attn_output, att_scores = self.att(inputs, inputs, inputs, return_attention_scores=True)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output), att_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZVNAC1FYiRB"
      },
      "source": [
        "def fft2d(x, shift_freq=False):   \n",
        "    fft2d = tf.math.real(tf.signal.fft2d(tf.cast(x, tf.complex64)))\n",
        "    if shift_freq:\n",
        "        fft2d = tf.signal.fftshift(fft2d, axes=(-1, -2))\n",
        "    return fft2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fft2dI(x, shift_freq=False):   \n",
        "    fft2dI = tf.math.imag(tf.signal.fft2d(tf.cast(x, tf.complex64)))\n",
        "    if shift_freq:\n",
        "        fft2dI = tf.signal.fftshift(fft2dI, axes=(-1, -2))\n",
        "    return fft2dI"
      ],
      "metadata": {
        "id": "NxNF9RYnisya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLHuXhGGYWlq"
      },
      "source": [
        "class FNETBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, rate=0.1, compression=False, shift_freq=False, asymmetric=False):\n",
        "        super(FNETBlock, self).__init__()\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        # self.dropout1 = layers.Dropout(rate)\n",
        "        # self.dropout2 = layers.Dropout(rate)\n",
        "        self.compression = compression\n",
        "        self.shift_freq  = shift_freq\n",
        "        self.asymmetric  = asymmetric\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        fft2D = fft2d(inputs, shift_freq=self.shift_freq)\n",
        "        #fft2D = self.dropout1(fft2D, training=training)\n",
        "        out1 = self.layernorm1(inputs + fft2D)\n",
        "\n",
        "        if self.shift_freq:\n",
        "            if self.compression is not False:\n",
        "                if self.asymmetric:\n",
        "                    cut = tf.cast((out1.shape[-2] * self.compression)//2, tf.int32)\n",
        "                    out1 = out1[:, out1.shape[-2]//2:-cut, :]\n",
        "                else:\n",
        "                    cut = tf.cast((out1.shape[-2] * self.compression)//2, tf.int32)\n",
        "                    out1 = out1[:, cut:-cut, :]\n",
        "        else:\n",
        "            if self.compression is not False:\n",
        "                cut = tf.cast((out1.shape[-2] * (1-self.compression))//2, tf.int32)\n",
        "                outA = out1[:, :cut+1, :]   \n",
        "                outB = out1[:, -cut-1:, :]  \n",
        "                out1 = tf.concat([outA, outB], 1)             \n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        #ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChiBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, rate=0.1, compression=False, shift_freq=False, asymmetric=False):\n",
        "        super(ChiBlock, self).__init__()\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.ffn2 = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm4 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.compression = compression\n",
        "        self.shift_freq  = shift_freq\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        fft2D  = fft2d(inputs, shift_freq=self.shift_freq)\n",
        "        fft2DI = fft2dI(inputs, shift_freq=self.shift_freq)\n",
        "\n",
        "        out1 = self.layernorm1(inputs + fft2D)\n",
        "        out2 = self.layernorm3(inputs + fft2DI)\n",
        "\n",
        "        if self.shift_freq:\n",
        "            if self.compression is not False:\n",
        "                cut = tf.cast((out1.shape[-2] * self.compression)//2, tf.int32)\n",
        "                out1 = out1[:, cut:-cut, :]\n",
        "                out2 = out2[:, cut:-cut, :]\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output2 = self.ffn2(out2)\n",
        "        return self.layernorm2(out1 + ffn_output), self.layernorm4(out2 + ffn_output2)"
      ],
      "metadata": {
        "id": "VQFrCWlvjAIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSA_KOCUTugY"
      },
      "source": [
        "class TimeSkipFNETBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, rate=0.1, compression=False, shift_freq=False, asymmetric=False):\n",
        "        super(TimeSkipFNETBlock, self).__init__()\n",
        "        self.ffn1 = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        if shift_freq:\n",
        "            self.ffn2 = tf.keras.Sequential(\n",
        "                [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "            )      \n",
        "            self.ffn3 = tf.keras.Sequential(\n",
        "                [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "            )\n",
        "        self.ffn4 = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )             \n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm4 = layers.LayerNormalization(epsilon=1e-6)   \n",
        "        self.layernorm5 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm6 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm7 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm8 = layers.LayerNormalization(epsilon=1e-6)                \n",
        "        self.compression = compression\n",
        "        self.shift_freq  = shift_freq\n",
        "\n",
        "    def call(self, inputs, training):      \n",
        "        fft2D = fft2d(inputs, shift_freq=self.shift_freq)\n",
        "        out1 = self.layernorm1(fft2D)\n",
        "        ffn_output = self.ffn1(out1)\n",
        "        finalout = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        if self.shift_freq:\n",
        "            fft2D = fft2d(finalout, shift_freq=self.shift_freq)\n",
        "            out1 = self.layernorm3(fft2D)\n",
        "            ffn_output = self.ffn2(out1)\n",
        "            finalout = self.layernorm4(out1 + ffn_output)\n",
        "\n",
        "            fft2D = fft2d(finalout, shift_freq=self.shift_freq)\n",
        "            out1 = self.layernorm5(fft2D)\n",
        "            ffn_output = self.ffn3(out1)\n",
        "            finalout = self.layernorm6(out1 + ffn_output)            \n",
        "\n",
        "        fft2D = fft2d(finalout, shift_freq=self.shift_freq)\n",
        "        out1 = self.layernorm7(fft2D+inputs)\n",
        "        ffn_output = self.ffn4(out1)\n",
        "        finalout = self.layernorm8(out1 + ffn_output)\n",
        "\n",
        "        return finalout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCMvNdRhchFb"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vnetQgIvfRc"
      },
      "source": [
        "class TokenAndPositionEmbedding2(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding2, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions, tf.expand_dims(positions, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lydV7dFtpAms"
      },
      "source": [
        "class TokenAndPositionEmbedding3(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, rate=0.1):\n",
        "        super(TokenAndPositionEmbedding3, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x) + positions\n",
        "        x = self.dropout(x, training=training)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDmNdway3R6y"
      },
      "source": [
        "class TokenAndPositionEmbedding4(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, rate=0.1):\n",
        "        super(TokenAndPositionEmbedding4, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_encoding = positional_encoding(maxlen,\n",
        "                                            embed_dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        self.d_model = embed_dim\n",
        "\n",
        "    def call(self, x, training):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.token_emb(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :maxlen, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwUw1arn6vyb"
      },
      "source": [
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, mask_embedding=False, w2v_init=\"uniform\", pad_to_0=False):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, \n",
        "                                          output_dim=embed_dim, \n",
        "                                          embeddings_initializer=w2v_init,\n",
        "                                          #embeddings_constraint=tf.keras.constraints.UnitNorm(axis=0),\n",
        "                                          )\n",
        "        self.mask_embedding = mask_embedding\n",
        "        self.embed_dim      = embed_dim\n",
        "        self.pad_to_0       = pad_to_0\n",
        "    def call(self, x):\n",
        "        if self.mask_embedding is not False:\n",
        "            mask = tf.cast(x != self.mask_embedding, np.float32)\n",
        "        x = self.token_emb(x)\n",
        "        if self.mask_embedding is not False:\n",
        "            mask_embedded = tf.tile(tf.expand_dims(mask, -1), [1, 1, self.embed_dim])\n",
        "            if self.pad_to_0:\n",
        "                return x*mask_embedded, mask\n",
        "            else:\n",
        "                return x, mask\n",
        "        else:\n",
        "            return x, tf.zeros([0])\n",
        "\n",
        "# print(\"orthogonal\")     \n",
        "# embedding = TokenEmbedding(10, 5, 5, mask_embedding=4)\n",
        "# x = tf.cast(np.random.randint(0, 5, size=(2, 20)), np.float32)\n",
        "# z, mask = embedding(x)\n",
        "# print(\"mask \\t\\t:\", mask)  \n",
        "# print(\"input \\t\\t:\", x)\n",
        "# print(\"embedding \\t\\t:\", z)  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19cPDo6rCwIA"
      },
      "source": [
        "#only new_embedding\n",
        "class PositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim, rate):\n",
        "        super(PositionEmbedding, self).__init__()\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        maxlen = tf.shape(x)[-2]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        x += self.pos_emb(positions)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3QL17fPj581"
      },
      "source": [
        "#only new_embedding\n",
        "class PositionEncoding2(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim, rate, tss=None, c1=10_000):\n",
        "        super(PositionEncoding2, self).__init__()\n",
        "        if tss == None:\n",
        "            self.pos_encoding = positional_encoding(maxlen, embed_dim, c1)\n",
        "        else:\n",
        "            self.pos_encoding = tss_positional_encoding(maxlen, embed_dim, tss)        \n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        self.d_model = embed_dim\n",
        "\n",
        "    def call(self, x, training):\n",
        "        maxlen = tf.shape(x)[-2]\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :maxlen, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnPyTC6PKSvd"
      },
      "source": [
        "#only new_embedding\n",
        "class RelativePositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim, rate):\n",
        "        super(RelativePositionEmbedding, self).__init__()\n",
        "        self.pos_emb_q = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.pos_emb_k = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.pos_emb_v = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        maxlen = tf.shape(x)[-2]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        x_q = x + self.pos_emb_q(positions)\n",
        "        x_k = x + self.pos_emb_k(positions)\n",
        "        x_v = x + self.pos_emb_v(positions)\n",
        "        x_q = self.dropout(x_q, training=training)\n",
        "        x_k = self.dropout(x_k, training=training)\n",
        "        x_v = self.dropout(x_v, training=training)\n",
        "        return x, x_q, x_k, x_v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffqf4GGyeR87"
      },
      "source": [
        "class Forward(layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super(Forward, self).__init__(name = name)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oe-sR2ANfq3"
      },
      "source": [
        "class FinalEmbedder(layers.Layer):\n",
        "    def __init__(self, kernelsMat, poolVec, embed_dim,):\n",
        "        super(FinalEmbedder, self).__init__()\n",
        "        self.embedding_network = [[layers.Conv1D(filters=embed_dim, kernel_size=k_size, strides=1, padding=\"same\", \n",
        "                                dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal') for k_size in kernelsMat[i]] for i in range(len(kernelsMat))]\n",
        "        self.poolings = [layers.AveragePooling1D(pool_size=pool_size, strides=None, padding=\"valid\") for pool_size in poolVec]  \n",
        "    def call(self, x, training):\n",
        "        for embedding_level, pooler in zip(self.embedding_network, self.poolings):\n",
        "            convoluted = []\n",
        "            for conv in embedding_level:\n",
        "                convoluted.append(conv(x))\n",
        "            if len(convoluted) > 0:\n",
        "                added = layers.Add()([x, *convoluted])\n",
        "            else:\n",
        "                added = x\n",
        "            pooled = pooler(added)\n",
        "            x = pooled        \n",
        "        return pooled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOsXLv_twmgU"
      },
      "source": [
        "class Add_REG(layers.Layer):\n",
        "    def __init__(self, embed_dim, rate=0.01, name=None):\n",
        "        super(Add_REG, self).__init__(name = name)\n",
        "        self.reg_emb = layers.Embedding(input_dim=1, output_dim=embed_dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training):\n",
        "        REG     = tf.range(start=0, limit=1, delta=1)\n",
        "        reg_emb = self.reg_emb(REG)\n",
        "        reg_emb = self.dropout(reg_emb, training=training)\n",
        "        reg_emb = tf.tile(tf.expand_dims(reg_emb, 0), [tf.shape(x)[0], 1, 1])\n",
        "        concat  = tf.concat([reg_emb, x], 1)\n",
        "        return concat\n",
        "\n",
        "# add_reg = Add_REG(32)\n",
        "# x = np.random.rand(4,5,32)\n",
        "# x = tf.cast(np.random.randint(0, 1, size=(4,5,32)), np.float32)\n",
        "# z = add_reg(x)\n",
        "# print(z.shape) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9e2bVrLZQx1"
      },
      "source": [
        "class prepare_AttentionMask(layers.Layer):\n",
        "    def __init__(self, add_reg, pool_size, name=None):\n",
        "        super(prepare_AttentionMask, self).__init__(name = name)\n",
        "        self.add_reg = add_reg\n",
        "        self.pool_size = pool_size\n",
        "    \n",
        "    def call(self, x, training):\n",
        "        x = tf.ones(tf.shape(x)) - x\n",
        "        x = tf.expand_dims(x, -1)\n",
        "        x = layers.MaxPool1D(pool_size=self.pool_size, strides=None, padding=\"valid\")(x) \n",
        "        if self.add_reg:\n",
        "            x = tf.concat([tf.zeros((tf.shape(x)[0], 1, 1)), x], axis=1) \n",
        "        x = tf.ones(tf.shape(x)) - x  \n",
        "        x = tf.matmul(x, x, transpose_b=True) \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class blend_buckets(layers.Layer):\n",
        "    def __init__(self, pool_size, name=None):\n",
        "        super(blend_buckets, self).__init__(name = name)\n",
        "        self.pool_size = pool_size\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        x, mask = inputs\n",
        "        mask = tf.ones(tf.shape(mask)) - mask\n",
        "        mask = tf.tile(tf.expand_dims(mask, -1), [1, 1, tf.shape(x)[-1]])\n",
        "        mask = layers.MaxPool1D(pool_size=self.pool_size, strides=None, padding=\"valid\")(mask) \n",
        "        mask = tf.ones(tf.shape(mask)) - mask  \n",
        "        return x*mask\n",
        "\n",
        "# print(\"orthogonal\")     \n",
        "# embedding = TokenEmbedding(10, 5, 5, mask_embedding=4)\n",
        "# x = tf.cast(np.random.randint(0, 5, size=(2, 6)), np.float32)\n",
        "# z, mask = embedding(x)\n",
        "# print(\"mask \\t\\t:\", mask)  \n",
        "# print(\"input \\t\\t:\", x)\n",
        "# print(\"embedding \\t\\t:\", z)  \n",
        "# avg = layers.AveragePooling1D(pool_size=2, strides=None, padding=\"valid\")(z)   \n",
        "# blend = blend_buckets(2)\n",
        "# result = blend([avg, mask])\n",
        "# print(\"result: \", result)"
      ],
      "metadata": {
        "id": "-2mCPfZrwRWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class add_Channel(layers.Layer):\n",
        "    def __init__(self, name=\"add_Channel\"):\n",
        "        super(add_Channel, self).__init__(name = name)\n",
        "    \n",
        "    def call(self, x, training):\n",
        "        x = tf.expand_dims(x, -1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "k-WILD4Q4StA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbzFgsGSW9nQ"
      },
      "source": [
        "def single_attentionplot(ax, catt, j, i):\n",
        "    ax.matshow(catt)\n",
        "    ax.set_xticks([int((catt.shape[0]-1)*i/6) for i in range(7)])\n",
        "    ax.set_yticks([int((catt.shape[0]-1)*i/6) for i in range(7)])                \n",
        "    #ax.grid(True, alpha=0.2)\n",
        "    if i == 0 and j==0:\n",
        "        ax.set_title(f\"Layer{j+1}\\nHead{i+1}\",pad=20)\n",
        "    elif i==0:\n",
        "        ax.set_title(f\"Layer{j+1}\",pad=20)\n",
        "    elif j==0:\n",
        "        ax.set_title(f\"Head{i+1}\",pad=20)\n",
        "\n",
        "def attention_plot(atts, epoch, show=True):\n",
        "    collapsed_atts = []\n",
        "    for catts in atts:\n",
        "        collapsed_atts.append(np.mean(catts, axis=0, keepdims=True))    \n",
        "    n_heads = atts[0].shape[1]\n",
        "    gene=13\n",
        "    fig, axs = plt.subplots(len(atts), n_heads, figsize=(30,  int(12*(1+len(atts)/4))), squeeze=False, constrained_layout=True)\n",
        "    for j, catts in enumerate(collapsed_atts):\n",
        "        for i in range(n_heads):\n",
        "            single_attentionplot(axs[j, i], catts[0, i, :, :], j, i)\n",
        "        \n",
        "    fig.suptitle(f'Mean Attention Plot - Epoch: {epoch}', y=0.98)\n",
        "    plt.tight_layout()\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(f\"/tmp/att_plots/attention_plot_{epoch}\")\n",
        "\n",
        "# atts = np.random.rand(4, 8, 4, 666, 666)\n",
        "# attention_plot(atts, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Qg-iivAveK"
      },
      "source": [
        "def single_attentionplot_REG(ax, ax_histx, ax_histy, catt, j, i):\n",
        "    ax.matshow(catt)\n",
        "    ax.set_yticks([int((catt.shape[0]-1)*i/6) for i in range(7)])\n",
        "\n",
        "    ax_histx.matshow(catt[0:1, :])\n",
        "    ax_histx.set(aspect=(catt.shape[0])/13)\n",
        "    ax_histy.matshow(catt[:, 0:1])\n",
        "    ax_histy.set(aspect=(13/catt.shape[0]))\n",
        "\n",
        "    ax_histx.set_xticks([int((catt.shape[0]-1)*i/6) for i in range(7)])\n",
        "    ax_histx.set_yticks([0])\n",
        "    ax_histx.set_yticklabels(['[REG]'])\n",
        "\n",
        "    ax_histy.set_xticks([])\n",
        "    ax_histy.set_xticklabels([])\n",
        "    ax_histy.set_yticks([])\n",
        "\n",
        "    if i == 0 and j==0:\n",
        "        ax_histx.set_title(f\"Layer{j+1}\\nHead{i+1}\\n\",pad=20)\n",
        "    elif i==0:\n",
        "        ax_histx.set_title(f\"Layer{j+1}\\n\",pad=20)\n",
        "    elif j==0:\n",
        "        ax_histx.set_title(f\"Head{i+1}\\n\",pad=20)\n",
        "    ax.set_xticks([])\n",
        "\n",
        "    ########################################################\n",
        "def attention_plot_REG(atts, epoch, show=True):\n",
        "    plt.rcParams.update({'font.size': 16})\n",
        "    collapsed_atts = []\n",
        "    for catts in atts:\n",
        "        collapsed_atts.append(np.mean(catts, axis=0, keepdims=True))\n",
        "\n",
        "    n_heads = atts[0].shape[1]\n",
        "    matdim = atts[0].shape[2]-1\n",
        "    gene=13\n",
        "    left, width = 0.1, 0.65\n",
        "    bottom, height = 0.1, 0.65\n",
        "    spacing = 0.000\n",
        "    fig = plt.figure(figsize=(10,  10))\n",
        "    for j, catts in enumerate(collapsed_atts):\n",
        "        for i in range(n_heads):\n",
        "            leftslack = 0.8*i\n",
        "            bottomslack = 0.8*j\n",
        "            rect_scatter = [left + leftslack,                   bottom - bottomslack,                    width, height]\n",
        "            rect_histx   = [left + leftslack,                   bottom + height + spacing - bottomslack, width, 0.05]\n",
        "            rect_histy   = [left + width + spacing + leftslack, bottom - bottomslack,                    0.05,  height] \n",
        "            ax       = fig.add_axes(rect_scatter)\n",
        "            ax_histx = fig.add_axes(rect_histx)\n",
        "            ax_histy = fig.add_axes(rect_histy)\n",
        "            single_attentionplot_REG(ax, ax_histx, ax_histy, catts[0, i, :, :], j, i)\n",
        "        \n",
        "    fig.suptitle(f'Mean Attention Plot - Epoch: {epoch}')\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(f\"/tmp/att_plots/attention_plot_REG_{epoch}\")\n",
        "\n",
        "# atts = np.random.rand(4, 8, 4, 667, 667)\n",
        "# attention_plot_REG(atts, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PISGMht0xUue"
      },
      "source": [
        "class projTransformer:\n",
        "    def __init__(   self,\n",
        "                    checkpoint_dir=\"\",\n",
        "                    model_type=\"best\",\n",
        "                    n_epochs=300, \n",
        "                    batch_size=32, \n",
        "                    learning_rate=1e-4,\n",
        "                    momentum=0.9,\n",
        "                    maxlen=10500,\n",
        "                    embed_dim=32,\n",
        "                    num_heads=4,\n",
        "                    ff_dim=64,\n",
        "                    vocab_size=5,\n",
        "                    dense=64,\n",
        "                    lr_reduction_epoch=None,\n",
        "                    dropout_rate=0.1,\n",
        "                    t_rate = 0.1,\n",
        "                    patience=20,\n",
        "                    optimizer=\"SGD\",\n",
        "                    warmup_steps = 64*20,\n",
        "                    shuffle = True,\n",
        "                    loss = \"mse\",\n",
        "                    logdir=None,\n",
        "                    n_layers1=3,\n",
        "                    n_layers2=0,\n",
        "                    compression=False,\n",
        "                    plot_attention=False,\n",
        "                    k_size = 6,\n",
        "                    pooler = \"global\",\n",
        "                    shift_freq = False,\n",
        "                    alternate = None,\n",
        "                    timeskip = False,\n",
        "                    p_enc=\"w2v\",\n",
        "                    convDownScale=True,\n",
        "                    tss=None,\n",
        "                    pool_size=30,\n",
        "                    add_reg=False,\n",
        "                    steps_per_epoch=5,\n",
        "                    reduction_factor = 5,\n",
        "                    mask_embedding = False,\n",
        "                    w2v_init = \"uniform\",\n",
        "                    output_neurons = 1,\n",
        "                    oneHot=False,\n",
        "                    n_targets=None,\n",
        "                    cardinality=16_358,\n",
        "                    normalization = \"batch\", # \"layer\" # \"batch\"\n",
        "                    chimera = False,\n",
        "                    halflife=False,\n",
        "                    asymmetric = False,\n",
        "                    pad_to_0 = False,\n",
        "                    DNA_tags = False,\n",
        "                    w2v_embdim = None,\n",
        "                    c1_posenc = 10_000,\n",
        "                    ):\n",
        "        \n",
        "        self.checkpoint_dir     = checkpoint_dir\n",
        "        self.model_type         = model_type\n",
        "        self.n_epochs           = n_epochs\n",
        "        self.batch_size         = batch_size\n",
        "        self.learning_rate      = learning_rate\n",
        "        self.momentum           = momentum\n",
        "        self.maxlen             = maxlen\n",
        "        self.embed_dim          = embed_dim\n",
        "        self.num_heads          = num_heads\n",
        "        self.ff_dim             = ff_dim\n",
        "        self.vocab_size         = vocab_size\n",
        "        self.dense              = dense\n",
        "        self.dropout_rate       = dropout_rate\n",
        "        self.lr_reduction_epoch = lr_reduction_epoch\n",
        "        self.t_rate             = t_rate\n",
        "        self.patience           = patience\n",
        "        self.optimizer          = optimizer\n",
        "        self.warmup_steps       = warmup_steps\n",
        "        self.shuffle            = shuffle\n",
        "        self.logdir             = logdir\n",
        "        self.loss               = loss\n",
        "        self.history            = \"\"\n",
        "        self.n_layers1          = n_layers1\n",
        "        self.n_layers2          = n_layers2\n",
        "        self.compression        = compression\n",
        "        self.plot_attention     = plot_attention\n",
        "        self.k_size             = k_size\n",
        "        self.p_enc              = p_enc\n",
        "        self.pooler             = pooler\n",
        "        self.alternate          = alternate\n",
        "        self.timeskip           = timeskip\n",
        "        self.shift_freq         = shift_freq\n",
        "        self.convDownScale      = convDownScale\n",
        "        self.tss                = tss\n",
        "        self.pool_size          = pool_size\n",
        "        self.steps_per_epoch    = steps_per_epoch\n",
        "        self.add_reg            = add_reg\n",
        "        self.reduction_factor   = reduction_factor\n",
        "        self.mask_embedding     = mask_embedding\n",
        "        self.w2v_init           = w2v_init\n",
        "        self.output_neurons     = output_neurons\n",
        "        self.oneHot             = oneHot\n",
        "        self.n_targets          = n_targets\n",
        "        self.cardinality        = cardinality\n",
        "        self.normalization      = normalization\n",
        "        self.chimera            = chimera\n",
        "        self.halflife           = halflife\n",
        "        self.asymmetric         = asymmetric\n",
        "        self.pad_to_0           = pad_to_0\n",
        "        self.DNA_tags           = DNA_tags\n",
        "        self.c1_posenc          = c1_posenc\n",
        "        if w2v_embdim == None:\n",
        "            self.w2v_embdim = embed_dim\n",
        "        else:\n",
        "            self.w2v_embdim = w2v_embdim\n",
        "        if self.chimera==True:\n",
        "            self.n_layers1 = 1\n",
        "            self.n_layers2 = 2            \n",
        "        ######################   \n",
        "        self._build_model()\n",
        "        ######################   \n",
        "\n",
        "\n",
        "        if self.n_layers2 == 0:\n",
        "            self.plot_attention = False\n",
        "\n",
        "        #self.learning_rate = tf.Variable(self.learning_rate, trainable=False, name='learning_rate')\n",
        "        #optimizer\n",
        "        if self.optimizer == \"Adam\":\n",
        "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        if self.optimizer == \"SGD\":\n",
        "            self.optimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum)\n",
        "        if self.optimizer == \"Adadelta\":\n",
        "            self.optimizer = tf.keras.optimizers.Adadelta(learning_rate=self.learning_rate, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "        if self.optimizer == \"Adamax\":\n",
        "            self.optimizer = tf.keras.optimizers.Adamax(learning_rate=self.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\")\n",
        "        if self.optimizer == \"Original\":\n",
        "            learning_rate = CustomSchedule(self.embed_dim, self.warmup_steps)\n",
        "            self.optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "        if self.optimizer == \"BERTAdam\":\n",
        "            # Set up epochs and steps\n",
        "            train_data_size = self.cardinality\n",
        "            steps_per_epoch = int(train_data_size / self.batch_size)\n",
        "            num_train_steps = steps_per_epoch * self.n_epochs\n",
        "            warmup_steps = int(self.n_epochs * train_data_size * 0.1 / self.batch_size)        \n",
        "            # creates an optimizer with learning rate schedule\n",
        "            self.optimizer = nlp.optimization.create_optimizer(\n",
        "                self.learning_rate, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)  \n",
        "        #self.loss =  tf.keras.losses.poisson    \n",
        "        if self.loss == \"mse\":   \n",
        "            self.loss =  tf.keras.losses.mean_squared_error \n",
        "        else:\n",
        "            self.loss =  tf.keras.losses.poisson  \n",
        "        # self.training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
        "        # self.training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('training_accuracy', dtype=tf.float32)          \n",
        "        # compile\n",
        "        #self.model.compile(optimizer=optimizer, loss=self.loss)\n",
        "        #self.model.compile(optimizer=optimizer, loss=custom_loss_function)\n",
        "\n",
        "    def _build_model(self):\n",
        "        if self.model_type == \"FNET_Final\":            \n",
        "            embedding_layer = TokenEmbedding(self.maxlen, self.vocab_size, self.w2v_embdim, self.mask_embedding, self.w2v_init, self.pad_to_0)\n",
        "            pooler = layers.Dense(self.embed_dim)\n",
        "\n",
        "            if self.timeskip:\n",
        "                FBlock = TimeSkipFNETBlock\n",
        "            elif self.chimera:\n",
        "                FBlock = ChiBlock\n",
        "            else:\n",
        "                FBlock = FNETBlock\n",
        "            if self.alternate is None:\n",
        "                fnet_layers  = [FBlock(self.embed_dim, self.ff_dim, self.t_rate, self.compression, self.shift_freq, self.asymmetric) for _ in range(self.n_layers1)]\n",
        "                trans_layers = [TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate, self.p_enc) for _ in range(self.n_layers2)]\n",
        "            else:\n",
        "                alternate_layers = []\n",
        "                for _ in range(self.n_layers1):\n",
        "                    for i, num in enumerate(self.alternate):\n",
        "                        if i % 2 == 0:\n",
        "                            for i in range(num):\n",
        "                                alternate_layers.append(FBlock(self.embed_dim, self.ff_dim, self.t_rate, self.compression, self.shift_freq, self.asymmetric)) \n",
        "                        else:\n",
        "                            for i in range(num):\n",
        "                                alternate_layers.append(TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate, self.p_enc))\n",
        "            #inputs\n",
        "            inputs = []\n",
        "\n",
        "\n",
        "            if self.oneHot:\n",
        "                input1 = layers.Input(shape=(self.maxlen, 4), name=\"sequence\")\n",
        "                embedded = layers.Conv1D(filters=self.embed_dim, kernel_size=1, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(input1)\n",
        "                inputs.append(input1)\n",
        "            else:\n",
        "                input1 = layers.Input(shape=(self.maxlen), name=\"sequence\")\n",
        "                inputs.append(input1)\n",
        "            \n",
        "            if self.halflife:\n",
        "                halflife = layers.Input(shape=(8), name=\"halflife\")\n",
        "                inputs.append(halflife)\n",
        "\n",
        "            if self.DNA_tags:\n",
        "                DNA_tags = layers.Input(shape=(512), name=\"dna_tags\")\n",
        "                inputs.append(DNA_tags)\n",
        "                dna_embedding = layers.Embedding(self.vocab_size, self.embed_dim)\n",
        "                dna_tags_embedded = dna_embedding(DNA_tags)\n",
        "                # dna_tags_embedded = layers.BatchNormalization()(dna_tags_embedded)\n",
        "\n",
        "            #embedding\n",
        "            if self.oneHot == False:\n",
        "                embedded, masked = embedding_layer(input1)\n",
        "                if self.w2v_embdim != self.embed_dim:\n",
        "                    embedded = layers.Conv1D(filters=self.embed_dim, kernel_size=1, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                \n",
        " \n",
        "            #cnn layers\n",
        "            if self.convDownScale:     \n",
        "                if self.k_size == \"multiconv\":  \n",
        "                    x1 = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    x2 = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)                                                                \n",
        "                    skip = layers.Add()([x1, x2, embedded])     \n",
        "                elif  self.k_size == \"multiconv_proj\": \n",
        "                    x1 = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    x2 = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)   \n",
        "                    x12 = layers.Concatenate()([x1,x2])\n",
        "                    x12 = layers.Dense(self.embed_dim, activation=\"relu\")(x12)\n",
        "                    #x12 = layers.Dropout(self.dropout_rate)(x12)                                                            \n",
        "                    skip = layers.Add()([x12, embedded]) \n",
        "                elif  self.k_size == \"multiconv_proj_light\": \n",
        "                    x1 = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    x2 = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)   \n",
        "                    x1 = layers.Dense(self.embed_dim, activation=\"relu\")(x1)\n",
        "                    x2 = layers.Dense(self.embed_dim, activation=\"relu\")(x2)\n",
        "                    #x12 = layers.Dropout(self.dropout_rate)(x12)                                                            \n",
        "                    skip = layers.Add()([x1, x2, embedded])                     \n",
        "                elif  self.k_size == \"head_conv\": \n",
        "                    x1 = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=self.num_heads, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    x2 = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=self.num_heads, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)                                                                \n",
        "                    skip = layers.Add()([x1, x2, embedded])                       \n",
        "                    \n",
        "                else:\n",
        "                    x = layers.Conv1D(filters=self.embed_dim, kernel_size=self.k_size, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded) \n",
        "                    skip = layers.Add()([x, embedded])  \n",
        "                     \n",
        "                x = layers.AveragePooling1D(pool_size=self.pool_size, strides=None, padding=\"valid\")(skip)   \n",
        "            else:\n",
        "                x = embedded  \n",
        "\n",
        "            if self.DNA_tags:\n",
        "                x = layers.Add()([x, dna_tags_embedded])   \n",
        "\n",
        "            if self.normalization == \"batch\":\n",
        "                x = layers.BatchNormalization()(x)\n",
        "            elif self.normalization == \"layer\":\n",
        "                x = layers.LayerNormalization()(x)\n",
        "\n",
        "            posemblen = x.shape[1]         \n",
        "            if self.p_enc == \"w2v\":\n",
        "                posenc_layer = PositionEmbedding(posemblen, self.embed_dim, self.t_rate)\n",
        "            elif self.p_enc == \"relative\":\n",
        "                posenc_layer = RelativePositionEmbedding(posemblen, self.embed_dim, self.t_rate)\n",
        "            else:\n",
        "                posenc_layer = PositionEncoding2(posemblen, self.embed_dim, self.t_rate, self.tss, self.c1_posenc)\n",
        "\n",
        "            x = posenc_layer(x)\n",
        "            if self.mask_embedding is not False and self.n_layers1==0:\n",
        "                masked = prepare_AttentionMask(self.add_reg, self.pool_size)(masked)\n",
        "            else:\n",
        "                if self.pad_to_0:\n",
        "                    x = blend_buckets(self.pool_size)([x, masked])\n",
        "                masked = None\n",
        "\n",
        "            if self.compression == False:\n",
        "                if self.add_reg:\n",
        "                    add_reg = Add_REG(self.embed_dim)\n",
        "                    x = add_reg(x) \n",
        "\n",
        "            att_scores = []\n",
        "            #transformers\n",
        "            if self.chimera:\n",
        "                x, x2 = fnet_layers[0](x)\n",
        "                if self.compression != False:\n",
        "                    if self.add_reg:\n",
        "                        x = Add_REG(self.embed_dim)(x)\n",
        "                        x2 = Add_REG(self.embed_dim)(x2)\n",
        "                x, atts  = trans_layers[0](x)\n",
        "                att_scores.append(atts)\n",
        "                x2, atts = trans_layers[1](x2)\n",
        "                att_scores.append(atts)\n",
        "            elif self.alternate is None:\n",
        "                for i in range(self.n_layers1):\n",
        "                    x = fnet_layers[i](x)\n",
        "                ###########################\n",
        "                if self.compression != False:\n",
        "                    if self.add_reg:\n",
        "                        add_reg = Add_REG(self.embed_dim)\n",
        "                        x = add_reg(x)\n",
        "                #############################\n",
        "                for i in range(self.n_layers2):\n",
        "                    x, atts = trans_layers[i](x, masked=masked)\n",
        "                    att_scores.append(atts)\n",
        "            else:\n",
        "                for layer in alternate_layers:\n",
        "                    if isinstance(layer, FBlock):\n",
        "                        x = layer(x)\n",
        "                    else:    \n",
        "                        x, atts = layer(x, masked=masked)\n",
        "                        att_scores.append(atts)         \n",
        " \n",
        "            #FC\n",
        "            if self.pooler == \"global\":\n",
        "                if self.chimera:\n",
        "                    x = layers.GlobalAveragePooling1D()(x)\n",
        "                    x2 = layers.GlobalAveragePooling1D()(x2)\n",
        "                    x = layers.Flatten()(x)\n",
        "                    x2 = layers.Flatten()(x2)\n",
        "                    x = layers.Concatenate()([x, x2])\n",
        "                else:\n",
        "                    x = layers.GlobalAveragePooling1D()(x)\n",
        "            else: # tanh\n",
        "                if self.chimera:\n",
        "                    x = pooler(x[:, 0])\n",
        "                    x2 = layers.Dense(self.embed_dim)(x2[:, 0])\n",
        "                    x = layers.Flatten()(x)\n",
        "                    x2 = layers.Flatten()(x2)\n",
        "                    x = layers.Concatenate()([x, x2])\n",
        "                else:\n",
        "                    x = pooler(x[:, 0])\n",
        "                    x = tf.keras.activations.tanh(x)\n",
        "            \n",
        "            if self.halflife:\n",
        "                x = layers.Concatenate()([x, halflife])\n",
        "            # x = layers.Flatten()(x)\n",
        "            #x = layers.Concatenate()([x, input2])\n",
        "            #dense1\n",
        "            x = layers.Dense(self.dense, activation=\"relu\")(x)\n",
        "            x = layers.Dropout(self.dropout_rate)(x)\n",
        "            #dense2\n",
        "            x = layers.Dense(self.dense, activation=\"gelu\")(x)\n",
        "            x = layers.Dropout(self.dropout_rate)(x)\n",
        "            #output\n",
        "            if self.n_targets == None:\n",
        "                output = layers.Dense(self.output_neurons, activation=\"linear\", name=\"Regression_Output\")(x)\n",
        "            else:\n",
        "                x = add_Channel()(x)\n",
        "                output = layers.Conv1D(filters=self.n_targets, kernel_size=1, strides=1, padding=\"same\", \n",
        "                                    dilation_rate=1, groups=1, activation=\"softplus\", kernel_initializer='glorot_normal')(x)\n",
        "\n",
        "            print(\"model built\")\n",
        "\n",
        "            if len(att_scores) > 0:\n",
        "                #att_scores = Forward(name=\"att_scores\")(att_scores)\n",
        "                self.model = tf.keras.Model(\n",
        "                    inputs=inputs,\n",
        "                    outputs={'Regression_Output': output, 'Attention_Scores': att_scores},\n",
        "                    )\n",
        "            else:\n",
        "                self.model = tf.keras.Model(\n",
        "                    inputs=inputs,\n",
        "                    outputs={'Regression_Output': output},\n",
        "                    )\n",
        "            self.model.summary()\n",
        "            img = tf.keras.utils.plot_model(self.model, f\"{self.model_type}.png\", show_shapes=True)\n",
        "            display(img)             \n",
        "\n",
        "        print(f\"\\nParameters:\\n\")\n",
        "        for k, v in vars(self).items():\n",
        "            pad = ' '.join(['' for _ in range(25-len(k))])\n",
        "            print(k, f\" :{pad}\", v)\n",
        "  \n",
        "\n",
        "    def train_model(self, x_train, x_val, TPU=False, cardinality=16_000, strategy=None, show_att=True):        \n",
        "\n",
        "        def create_step_function(model, optimizer):\n",
        "            @tf.function\n",
        "            def train_step(batch, head='Regression_Output', optimizer_clip_norm_global=0.2):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    if self.halflife:\n",
        "                        inputs = [batch['sequence'], batch['halflife']]\n",
        "                    elif self.DNA_tags:\n",
        "                        inputs = [batch['sequence'], batch['dna_tags']]\n",
        "                    else:\n",
        "                        inputs = [batch['sequence']]\n",
        "                    outputs     = model(inputs, training=True)[head]\n",
        "                    vec_loss    = self.loss(batch['target'], outputs)\n",
        "                    loss        = tf.reduce_mean(vec_loss)\n",
        "                \n",
        "                gradients = tape.gradient(loss, model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))  #   skip_gradients_aggregation=False\n",
        "\n",
        "                return vec_loss, outputs\n",
        "            return train_step  \n",
        "\n",
        "        def eval_step_function(model):\n",
        "            @tf.function\n",
        "            def eval_step(batch, head='Regression_Output', optimizer_clip_norm_global=0.2):\n",
        "                if self.halflife:\n",
        "                    inputs = [batch['sequence'], batch['halflife']]\n",
        "                elif self.DNA_tags:\n",
        "                    inputs = [batch['sequence'], batch['dna_tags']]\n",
        "                else:\n",
        "                    inputs = [batch['sequence']]\n",
        "                outputs     = model(inputs, training=False)\n",
        "                if self.plot_attention:\n",
        "                    att_scores  = outputs['Attention_Scores']\n",
        "                else:\n",
        "                    att_scores = None\n",
        "                vec_loss    = self.loss(batch['target'], outputs[head])\n",
        "                return vec_loss, att_scores\n",
        "            return eval_step         \n",
        "        \n",
        "        train_step = create_step_function(self.model, self.optimizer)\n",
        "        eval_step  = eval_step_function(self.model)\n",
        "        # Train the model\n",
        "        print(\"cardinality\", cardinality)\n",
        "        steps_per_epoch = cardinality//self.batch_size        \n",
        "        train_data_it   = iter(x_train)\n",
        "\n",
        "        global_step = 0\n",
        "        lowest_val_loss = np.Inf\n",
        "        patience_counter = 0\n",
        "\n",
        "        STOP = False\n",
        "\n",
        "        #first attention plot\n",
        "        if self.plot_attention:\n",
        "            batch_val = next(iter(x_val))\n",
        "            if strategy is None:\n",
        "                _, att_sco = eval_step(batch_val)\n",
        "            else:\n",
        "                _, att_sco = strategy.run(eval_step, args=(batch_val,))\n",
        "                att_sco       = strategy.gather(att_sco, axis=None)\n",
        "            if self.add_reg:\n",
        "                attention_plot_REG(att_sco, 0, show_att)\n",
        "            else:\n",
        "                attention_plot(att_sco, 0, show_att)\n",
        "\n",
        "        self.history = {'train_loss' : [], 'val_loss': [], 'epoch': []}     \n",
        "        hasattrNumpy = hasattr(self.optimizer.learning_rate, 'numpy')   \n",
        "        # training            \n",
        "        for epoch_i in range(self.n_epochs):\n",
        "            if STOP:\n",
        "                break\n",
        "            print(\"\\n\", f'Epoch: {epoch_i+1}',)\n",
        "            train_loss_vec = []\n",
        "            val_loss_vec   = []     \n",
        "            # lr scheduler       \n",
        "            if epoch_i == self.lr_reduction_epoch:\n",
        "                if hasattrNumpy:\n",
        "                    self.learning_rate = self.learning_rate/self.reduction_factor\n",
        "                    self.optimizer.learning_rate.assign(self.learning_rate)\n",
        "                    print(\"New Learning Rate Assigned: \", self.learning_rate)\n",
        "            for i in tqdm(range(steps_per_epoch)):                \n",
        "                global_step += 1\n",
        "\n",
        "                # if global_step > 1:\n",
        "                #     learning_rate_frac = tf.math.minimum(\n",
        "                #         1.0, global_step / tf.math.maximum(1.0, num_warmup_steps))      \n",
        "                #     learning_rate.assign(target_learning_rate * learning_rate_frac)\n",
        "\n",
        "                batch_train = next(train_data_it)\n",
        "                if strategy is None:\n",
        "                    train_loss, _     = train_step(batch=batch_train)\n",
        "                else:\n",
        "                    train_loss, o     = strategy.run(train_step, args=(batch_train,))\n",
        "                    train_loss        = strategy.gather(train_loss, axis=0) \n",
        "                    #print(\"train_loss: \", train_loss)\n",
        "                train_loss_vec.append(np.mean(train_loss.numpy()))\n",
        "            \n",
        "            for i, batch_val in enumerate(x_val):                \n",
        "                if strategy is None:\n",
        "                    loss, att_sco = eval_step(batch_val)\n",
        "                else:\n",
        "                    loss, att_sco = strategy.run(eval_step, args=(batch_val,))\n",
        "                    loss          = strategy.gather(loss, axis=0)\n",
        "                    att_sco       = strategy.gather(att_sco, axis=None)\n",
        "                if i == 0:\n",
        "                    atts = att_sco                \n",
        "                val_loss_vec.append(np.mean(loss.numpy()))\n",
        "\n",
        "            # End of epoch.\n",
        "            train_loss = round(np.array(train_loss_vec).mean(), 3)\n",
        "            val_loss   = round(np.array(val_loss_vec).mean(), 3)\n",
        "            if hasattrNumpy:\n",
        "                lr = self.optimizer.learning_rate.numpy()                \n",
        "            else:                \n",
        "                lr = self.optimizer.learning_rate(global_step)\n",
        "            print(\n",
        "                    '\\t\\ttrain_loss: %.3f'  %train_loss,\n",
        "                    '\\tval_loss: %.3f'      %val_loss,\n",
        "                    '\\tlearning_rate: %.3E' %lr\n",
        "                 )\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['epoch'].append(epoch_i)\n",
        "            # patience\n",
        "            if val_loss < lowest_val_loss:\n",
        "                lowest_val_loss = val_loss\n",
        "                lowest_val_loss_epoch = epoch_i+1\n",
        "                patience_counter = 0               \n",
        "                print(\"############################################ New lowest_val_loss reached #########################\") \n",
        "                # Saving the model to a path on localhost.\n",
        "                saved_model_path = \"/tmp/tf_save\"\n",
        "                save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "                self.model.save(saved_model_path, options=save_options)\n",
        "            elif patience_counter == self.patience:\n",
        "                print(  f\"End of training phase - Patience threshold reached\",\n",
        "                        f'\\nWeights Restored from Lowest val_loss epoch: {lowest_val_loss_epoch}',\n",
        "                        f'\\nlowest_val_loss: {lowest_val_loss}')\n",
        "                STOP = True\n",
        "            else:\n",
        "                patience_counter += 1            \n",
        "            # attention plot\n",
        "            if self.plot_attention and epoch_i%10==0:\n",
        "                if self.add_reg:\n",
        "                    attention_plot_REG(atts, epoch_i+1, show_att)\n",
        "                else:\n",
        "                    attention_plot(atts, epoch_i+1, show_att)                \n",
        "        \n",
        "        fig, ax = plt.subplots(1, figsize=(20,9))\n",
        "        ax.plot(self.history['epoch'], self.history['train_loss'], color=\"tab:blue\")\n",
        "        ax.plot(self.history['epoch'], self.history['val_loss'], color=\"tab:orange\")\n",
        "        ax.axhline(0.42, 0, len(self.history['epoch']), alpha=0.2)\n",
        "        ax.axhline(0.40, 0, len(self.history['epoch']), alpha=0.2)\n",
        "        ax.axhline(0.36, 0, len(self.history['epoch']), alpha=0.2)\n",
        "        ax.set_xlabel(\"epoch\")\n",
        "        ax.set_ylabel(\"loss\")\n",
        "        ax.set_title('model train vs validation loss')\n",
        "        blue_patch  = mpatches.Patch(color='tab:blue', label='train')\n",
        "        orange_patch = mpatches.Patch(color='tab:orange', label='validation')\n",
        "        patches = [blue_patch, orange_patch]   \n",
        "        plt.legend(handles=patches, loc='upper right')             \n",
        "        fig.show()\n",
        "\n",
        "    def evaluate_model(self, dataset, head='Regression_Output', strategy = None):\n",
        "        def test_step_function(model):\n",
        "            @tf.function\n",
        "            def test_step(batch, head='Regression_Output', optimizer_clip_norm_global=0.2):\n",
        "                if self.halflife:\n",
        "                    inputs = [batch['sequence'], batch['halflife']]\n",
        "                elif self.DNA_tags:\n",
        "                    inputs = [batch['sequence'], batch['dna_tags']]\n",
        "                else:\n",
        "                    inputs = [batch['sequence']]\n",
        "                output     = model(inputs, training=False)\n",
        "                return output[head]\n",
        "            return test_step \n",
        "\n",
        "        test_step = test_step_function(self.model)  \n",
        "        predictions  = []\n",
        "        y            = []\n",
        "        for batch_test in dataset:\n",
        "            if strategy is None:\n",
        "                output          = test_step(batch_test)  \n",
        "                predictions  += list(output.numpy().flatten())        \n",
        "                y            += list(batch_test['target'].numpy().flatten())                              \n",
        "            else:\n",
        "                output          = strategy.run(test_step, args=(batch_test,))\n",
        "                output          = strategy.gather(output, axis=0)\n",
        "                batch_test      = strategy.gather(batch_test['target'], axis=0)  \n",
        "                predictions  += list(output.numpy().flatten())        \n",
        "                y            += list(batch_test.numpy().flatten())\n",
        "\n",
        "        print('y: ', len(y))\n",
        "        print('pred: ', len(predictions))\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n",
        "        pearson, _ = stats.mstats.pearsonr(predictions, y)\n",
        "        print('Test R^2 = %.3f' % r_value**2)\n",
        "        print('Test Pearson = %.3f' % pearson)\n",
        "        spearman = stats.spearmanr(predictions, y)[0]\n",
        "        print('Test Spearman = %.3f' % spearman)\n",
        "        return predictions, y\n",
        "\n",
        "    def evaluate_best_model(self, dataset, head='Regression_Output', strategy = None):\n",
        "        def test_step_function(model):\n",
        "            @tf.function\n",
        "            def test_step(batch, head='Regression_Output', optimizer_clip_norm_global=0.2):\n",
        "                if self.halflife:\n",
        "                    inputs = [batch['sequence'], batch['halflife']]\n",
        "                elif self.DNA_tags:\n",
        "                    inputs = [batch['sequence'], batch['dna_tags']]\n",
        "                else:\n",
        "                    inputs = [batch['sequence']]\n",
        "                output     = model(inputs, training=False)\n",
        "                return output[head]\n",
        "            return test_step \n",
        "        # Loading the model from a path on localhost.\n",
        "        saved_model_path = \"/tmp/tf_save\"\n",
        "        if strategy is not None:\n",
        "            with strategy.scope():\n",
        "                load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "                best_model = tf.keras.models.load_model(saved_model_path, options=load_options)      \n",
        "        else:\n",
        "            load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "            best_model = tf.keras.models.load_model(saved_model_path, options=load_options)            \n",
        "\n",
        "        test_step = test_step_function(best_model)  \n",
        "        predictions  = []\n",
        "        y            = []\n",
        "        for batch_test in dataset:\n",
        "            if strategy is None:\n",
        "                output          = test_step(batch_test)  \n",
        "                predictions  += list(output.numpy().flatten())        \n",
        "                y            += list(batch_test['target'].numpy().flatten())                              \n",
        "            else:\n",
        "                output          = strategy.run(test_step, args=(batch_test,))\n",
        "                output          = strategy.gather(output, axis=0)\n",
        "                batch_test      = strategy.gather(batch_test['target'], axis=0)  \n",
        "                predictions  += list(output.numpy().flatten())        \n",
        "                y            += list(batch_test.numpy().flatten())\n",
        "\n",
        "        print('y: ', len(y))\n",
        "        print('pred: ', len(predictions))\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n",
        "        pearson, _ = stats.mstats.pearsonr(predictions, y)\n",
        "        print('Test R^2 = %.3f' % r_value**2)\n",
        "        print('Test Pearson = %.3f' % pearson)\n",
        "        spearman = stats.spearmanr(predictions, y)[0]\n",
        "        print('Test Spearman = %.3f' % spearman)\n",
        "        return predictions, y                \n",
        "\n",
        "    def plot_kde(self, dataset, head='Regression_Output', strategy = None, predictions=None, y=None):\n",
        "        if predictions == None and y == None:\n",
        "            predictions, y = self.evaluate_best_model(dataset, head, strategy)\n",
        "        df = pd.DataFrame({\"predictions\":predictions, \"true\":y})\n",
        "        ax = sns.displot(data=df, kde=True)\n",
        "        plt.xlabel(\"Labels\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_train(self, references=[0.36, 0.4, 0.42]):\n",
        "        fig, ax = plt.subplots(1, figsize=(20,9))\n",
        "        ax.plot(self.history['epoch'], self.history['train_loss'], color=\"tab:blue\")\n",
        "        ax.plot(self.history['epoch'], self.history['val_loss'], color=\"tab:orange\")\n",
        "        for ref in references:\n",
        "            ax.axhline(ref, 0, len(self.history['epoch']), alpha=0.2)\n",
        "        ax.set_xlabel(\"epoch\")\n",
        "        ax.set_ylabel(\"loss\")\n",
        "        ax.set_title('model train vs validation loss')\n",
        "        blue_patch  = mpatches.Patch(color='tab:blue', label='train')\n",
        "        orange_patch = mpatches.Patch(color='tab:orange', label='validation')\n",
        "        patches = [blue_patch, orange_patch]   \n",
        "        plt.legend(handles=patches, loc='upper right')             \n",
        "        fig.show()\n",
        "\n",
        "    def plot_r2(self, dataset, head='Regression_Output', strategy = None, predictions=None, y=None, ylim=(-1.5,3), xlim=(-1.5,3)):\n",
        "        if predictions == None and y == None:\n",
        "            predictions, y = self.evaluate_best_model(dataset, head, strategy)\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n",
        "        predictions = np.array(predictions)\n",
        "        y = np.array(y)\n",
        "        viridis = cm.get_cmap('autumn', 12)\n",
        "        diff = y - predictions\n",
        "        diff = np.abs(diff)\n",
        "\n",
        "        ### plt size\n",
        "        plt.rcParams[\"figure.figsize\"] = (10,9)\n",
        "        ### plt fontsize\n",
        "        plt.rcParams.update({'font.size': 16})\n",
        "\n",
        "        ### set title\n",
        "        plt.title(\"Expression Scatterplot\")\n",
        "        ### plot\n",
        "        bis = np.arange(-1.5, 3, 2)\n",
        "        plt.plot(bis, bis,  f\"b\", alpha=0.3)\n",
        "        for p, yi, c in zip(predictions, y, diff):\n",
        "            plt.plot(p, yi,  f\".\", markersize=10, color=viridis((1.0-c)/1.1))\n",
        "        ### set ticks\n",
        "        plt.xticks([i for i in range(-1, 4)])\n",
        "        plt.yticks([i for i in range(-1, 4)])\n",
        "        ### set labels\n",
        "        plt.xlabel(\"Predicted expression level\")\n",
        "        plt.ylabel(\"Median expression level\")\n",
        "        ### create legend\n",
        "        plt.legend(loc=\"upper right\", title=f\"r2 = %.3f\\n n = 1000\" % r_value**2)\n",
        "        ### set ylim\n",
        "        if ylim!=None:\n",
        "            plt.ylim(ylim)\n",
        "        if xlim!=None:\n",
        "            plt.xlim(xlim)\n",
        "        ### grid\n",
        "        plt.grid(alpha=0.5)\n",
        "        ### save\n",
        "        # if self.save:\n",
        "        #     plt.savefig(f\"{self.dir}{self.filename}.png\")\n",
        "        ### show\n",
        "        plt.show()\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _split_validation_data(x, y, validation_split):\n",
        "        rand_indexes = np.random.permutation(x.shape[0])\n",
        "        x = x[rand_indexes]\n",
        "        y = y[rand_indexes]\n",
        "        x_validation = x[:int(len(x) * validation_split)]\n",
        "        y_validation = y[:int(len(x) * validation_split)]\n",
        "        x_train = x[int(len(x) * validation_split):]\n",
        "        y_train = y[int(len(x) * validation_split):]\n",
        "        return x_train, y_train, x_validation, y_validation\n",
        "\n",
        "    def lr_scheduler(self, epoch, lr):\n",
        "        if epoch == self.lr_reduction_epoch:\n",
        "            return lr * 0.2\n",
        "        else:\n",
        "            return lr "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIW0GrW--WR7"
      },
      "source": [
        "maxlen=3000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX0FerWW46EQ"
      },
      "source": [
        "model_type      = \"FNET_Final\"\n",
        "logdir          = None\n",
        "loss            = \"mse\"\n",
        "p_enc           = \"original\"\n",
        "pooler          = \"tanh\"\n",
        "ff_dim          = 666\n",
        "n_layers1       = 1\n",
        "n_layers2       = 2\n",
        "shift_freq      = False\n",
        "plot_attention  = True\n",
        "alternate       = None\n",
        "timeskip        = False\n",
        "compression     = 0.95\n",
        "convDownScale   = True\n",
        "n_epochs        = 2\n",
        "batch_size      = 256\n",
        "add_reg         = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fs2sPAF2cHZ"
      },
      "source": [
        "# net = projTransformer(checkpoint_dir=\"FNET/\", model_type=model_type, n_epochs=n_epochs, batch_size=batch_size, alternate=alternate, add_reg=add_reg, #n_targets=2,\n",
        "#                       learning_rate=3e-4, patience=20, optimizer=\"Adam\", vocab_size=4, k_size = 6, pooler=pooler, shift_freq=shift_freq, timeskip=timeskip, chimera=False,\n",
        "#                       lr_reduction_epoch=60, maxlen=maxlen, embed_dim=32, num_heads=4, ff_dim=ff_dim, dense=64, plot_attention=plot_attention, convDownScale=convDownScale,\n",
        "#                       dropout_rate=0.1, logdir=logdir, t_rate=0.1, momentum=0.9, loss=loss, n_layers1=n_layers1, n_layers2=n_layers2, compression=compression, \n",
        "#                       p_enc=p_enc, mask_embedding=4, halflife=True, pad_to_0=True)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbjPZMePookE"
      },
      "source": [
        "# cardinality = 1000\n",
        "\n",
        "# X_trainpromoter_p    = np.random.randint(0, 3, size=(cardinality, maxlen))\n",
        "# X_trainhalflife      = np.random.rand(cardinality, 8)\n",
        "# y_train              = np.random.rand(cardinality)\n",
        "\n",
        "# X_testpromoter_p    = np.random.randint(0, 3, size=(cardinality, maxlen))\n",
        "# X_testhalflife      = np.random.rand(cardinality, 8)\n",
        "# y_test              = np.random.rand(cardinality)\n",
        "\n",
        "# x_train = tf.data.Dataset.from_tensor_slices(({'sequence': X_trainpromoter_p, 'halflife': X_trainhalflife, 'target': y_train.reshape(-1,1)}))\n",
        "# x_val   = tf.data.Dataset.from_tensor_slices(({'sequence': X_testpromoter_p, 'halflife': X_testhalflife, 'target': y_test.reshape(-1,1)}))\n",
        "\n",
        "# x_train = x_train.repeat().batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
        "# x_val   = x_val.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# # net.train_model(x_train, x_val, False, cardinality=cardinality)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inIkWp9f6UZ4",
        "outputId": "9cc7eb43-0a6d-4a2a-ac0b-5549d2508b1b"
      },
      "source": [
        "# net.evaluate_model(x_val)\n",
        "# print(\"\")\n",
        "# # net.plot_kde(x_val)\n",
        "# # print(\"\")\n",
        "# # net.plot_r2(x_val)\n",
        "# # print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y:  1000\n",
            "pred:  1000\n",
            "Test R^2 = 0.000\n",
            "Test Pearson = 0.012\n",
            "Test Spearman = 0.018\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXAYVH3uaBxB"
      },
      "source": [
        "# masked = tf.concat([tf.cast(np.random.randint(1, 2, size=(1, 4)), np.float32), tf.cast(np.random.randint(0, 1, size=(1, 4)), np.float32), \n",
        "#                     tf.cast(np.random.randint(1, 2, size=(1, 2)), np.float32)], -1)\n",
        "# masked = tf.concat([tf.cast(np.random.randint(1, 2, size=(1, 10)), np.float32), masked], 0)\n",
        "# print(\"1\", masked)\n",
        "# # masked = tf.ones(tf.shape(masked)) - masked\n",
        "# # print(masked)\n",
        "\n",
        "\n",
        "# masked = tf.ones(tf.shape(masked)) - masked\n",
        "# print(\"2\", masked)\n",
        "# masked = tf.expand_dims(masked, -1)\n",
        "# print(\"3\", masked)\n",
        "# masked = layers.MaxPool1D(pool_size=2, strides=None, padding=\"valid\")(masked)  \n",
        "# print(\"4\", masked)\n",
        "# masked = tf.ones(tf.shape(masked)) - masked  \n",
        "# print(\"5\", masked)\n",
        "# tf.matmul(masked, masked, transpose_b=True)\n",
        "# # masked = tf.expand_dims(masked, 1)\n",
        "# # print(\"6\", masked)\n",
        "# # masked = tf.tile(masked, [1, 4, 1, tf.shape(masked)[2]])\n",
        "# # print(\"7\", masked)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}