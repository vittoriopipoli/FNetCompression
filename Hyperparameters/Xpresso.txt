model_type  :               FNET_Final
n_epochs  :                 300
batch_size  :               256
learning_rate  :            0.0002
momentum  :                 0.9
maxlen  :                   20000
embed_dim  :                32
num_heads  :                4
ff_dim  :                   64
vocab_size  :               5
dense  :                    128
dropout_rate  :             0.1
lr_reduction_epoch  :       30
t_rate  :                   0.1
patience  :                 30
optimizer  :                BERTAdam
warmup_steps  :             1280
shuffle  :                  True
logdir  :                   None
loss  :                     mse
history  :                  
n_layers1  :                1
n_layers2  :                1
compression  :              0.95
plot_attention  :           False
k_size  :                   multiconv_proj
p_enc  :                    original
pooler  :                   tanh
alternate  :                None
timeskip  :                 False
shift_freq  :               True
convDownScale  :            True
tss  :                      None
pool_size  :                128
steps_per_epoch  :          5
add_reg  :                  True
reduction_factor  :         5
mask_embedding  :           False
w2v_init  :                 uniform
output_neurons  :           1
oneHot  :                   False
n_targets  :                None
cardinality  :              16358
normalization  :            batch
chimera  :                  False
halflife  :                 True
asymmetric  :               False
pad_to_0  :                 False
DNA_tags  :                 False
c1_posenc  :                10000
w2v_embdim  :               32